Principle
---------

Retrieve a (whole) website, analyse the output.
Using "wget" for website retrieval, as it resembles the requirements very well.


Environment
-----------
The script was tested on Arch Linux using python 3.4.3 and wget 1.16.3.


Parameters
----------
Use "./wipro-crawler.py -h" to get current list of supported parameters


Examples
--------

# Analyse links and images on ungleich.ch
./wipro-crawler.py ungleich.ch

# Analyse links and images on a cached version of ungleich.ch
./wipro-crawler.py --no-download ungleich.ch

Example output
--------------

./wipro-crawler.py -n ungleich.ch                   
INFO: Crawling domain ungleich.ch
INFO: Analysing domain: ungleich.ch
ungleich.ch/index.html
    Images
        img/logo_white.svg
        img/logo_200x200.svg
        img/team/5.jpg
        img/team/4.jpg
        [...]
    Links
        #page-top
        #page-top
        #services
        #portfolio
        #about
        #team
        http://blog.ungleich.ch
        #contact
        [...]


Bugs / Notes
------------
- No parsing of CSS and Javascript involved: Many references may be missing
- The whole website is cached locally and processed afterwards -> should be optimised for large sites
- Using -e robots=off for wget turns off robots.txt support and retrieves the whole website, 
  even if robots.txt is trying to disable it
- Using --delete-after with wget could be used for automatic cleanup, but requires instant analysis of
  every file
- Limited the files to be retrieved using extension .php, .html, .asp: The motivation is 
  to not download images, however it may exclude more content than required.
- Used logging module so logging can be changed for production use instead of print()
- Log level can be adjusted for production use using log.setLevel(LEVEL)
  See https://docs.python.org/3.4/library/logging.html
- For real production use the code should be abstracted into a class that can be easily reused
- Log level of wget is not adjusted to log levell of python script -> needs to be synchronised
- No http/https support - even though wget is capable of it, it was excluded because handling of
  insecure ssl certificates needs to be added and parsing of url (which is at the moment just the
  domain name)
- The output format can easily be changed to be JSON, XML, etc. - the format was chosen for easy use
  on the command line
